\item \points{30} {\bf Neural Networks: MNIST image classification}

In this problem, you will implement a simple neural network
to classify grayscale images of handwritten digits (0 - 9) from
the MNIST dataset. The dataset contains 60,000 training images and
10,000 testing images of handwritten digits, 0 - 9. Each image is
28$\times$28 pixels in size, and is generally represented as a flat
vector of 784 numbers. It also includes labels for each example, a number
indicating the actual digit (0 - 9) handwritten in that image. A sample of
a few such images are shown below.

\begin{center}
\includegraphics[scale=0.5]{mnist/mnist_plot}
\end{center}

The data and starter code for this problem can be found in

\begin{itemize}
\item \texttt{src/mnist/nn.py}
\item \texttt{src/mnist/images\_train.csv}
\item \texttt{src/mnist/labels\_train.csv}
\item \texttt{src/mnist/images\_test.csv}
\item \texttt{src/mnist/labels\_test.csv}
\end{itemize}

The dataset files are initially provided in compressed \texttt{.gz} format. Be sure to unzip them before use so that the files are accessible as \texttt{.csv} files. To unzip the files:
\begin{itemize}
\item On Mac/Linux: \texttt{gunzip images\_train.csv.gz} (repeat for each \texttt{.gz} file)
\item On Windows: Use a tool like 7-Zip, WinRAR, or the built-in extraction feature
\end{itemize}
The starter code splits the set
of 60,000 training images and labels into a set of 50,000 examples as
the training set, and 10,000 examples for dev set.

To start, you will implement a neural network with a single hidden layer
and cross entropy loss, and train it with the provided data set. You will use the
sigmoid function as activation for the hidden layer and use the cross-entropy loss for multi-class classification. For a single example $(x, y)$, the cross-entropy loss is given by:
%\tnote{change the notation here to make it more consistent with the lecture }
$$\ell_\textup{CE}(\bar{h}_{\theta}(x),y) = - \log\left(\frac{\exp(\bar{h}_{\theta}(x)_{y})}{\sum_{s=1}^{k}\exp({\bar{h}_{\theta}(x)}_s)}\right),$$
where $\bar{h}_{\theta}(x) \in \mathbb{R}^{k}$ denotes the logits, i.e., the output of the model on a training example $x$, and $\bar{h}_{\theta}(x)_y$ is the $y$-th coordinate of the vector $\bar{h}_{\theta}(x)$ (with $y \in \{1, \dots, k\}$ serving as an index).

%\tnote{let's call the one-hot vector $e_y$ so that we don't have to overload the notation. Can still keep the $\hat{y}$. The cross-entropy loss should be defined the same as in the lecture notes}


We have labeled data $(x^{(i)}, y^{(i)})_{i=1}^n$, where $x^{(i)} \in \mathbb{R}^d$, and $y^{(i)} \in \{1,\dots, k\}$ is the ground truth label. Note that, in this specific application, we pass in the \textit{flattened} images as input. In other words, if each original input image is in $\mathbb{R}^{r \times r}$, we have $x^{(i)} \in \mathbb{R}^{r^2}$.

Additionally, let $m$ be the number of hidden units in the neural network, so that weight matrices $W^{[1]} \in \mathbb{R}^{d \times m}$ and $W^{[2]} \in \mathbb{R}^{m \times k}$.\footnote{Please note that the dimension of the weight matrices is different from those in the lecture notes, but we also multiply ${W^{[1]}}^\top$ instead of $W^{[1]}$ in the matrix multiplication layer.  Such a change of notation is mostly for some consistence with the convention in the code.} We also have biases $b^{[1]} \in \mathbb{R}^m$ and $b^{[2]} \in \mathbb{R}^k$. The parameters of the model $\theta$ is $(W^{[1]},W^{[2]},b^{[1]},b^{[2]})$.

We define the following neural network with a single hidden layer, which we refer to as the \textbf{Linear Layer NN}. For a single input $x^{(i)}$, the forward propagation equations are:

\begin{align}
  a^{(i)} &= \sigma \left( {W^{[1]}}^\top x^{(i)}  + b^{[1]} \right)  \in \mathbb{R}^m \nonumber \\
  \bar{h}_{\theta}(x^{(i)})&= {W^{[2]}}^\top a^{(i)} + b^{[2]} \in \mathbb{R}^k \nonumber \\
  {h}_{\theta}(x^{(i)}) &=  \mathrm{softmax}(\bar{h}_{\theta}(x^{(i)})) \in \mathbb{R}^k \nonumber
\end{align}
where $\sigma$ is the sigmoid function. The softmax function maps $\mathbb{R}^k \to \mathbb{R}^k$ and is defined as:
$$\mathrm{softmax}(z)_i = \frac{\exp(z_i)}{\sum_{j=1}^{k}\exp(z_j)}$$
for each component $i \in \{1, \dots, k\}$. 

For $\nexp$ training examples, we average the cross entropy loss over the $\nexp$ examples.
  \begin{equation*}
  J(W^{[1]},W^{[2]},b^{[1]},b^{[2]}) = \frac{1}{\nexp}\sum_{i=1}^\nexp \ell_\textup{CE}(\bar{h}_{\theta}(x^{(i)}),y^{(i)})  = - \frac{1}{\nexp}\sum_{i=1}^\nexp \log\left(\frac{\exp(\bar{h}_{\theta}(x^{(i)})_{y^{(i)}})}{\sum_{s=1}^{k}\exp({\bar{h}_{\theta}(x^{(i)})}_s)}\right).
  \end{equation*}

Suppose $e_y\in \Re^k$ is the one-hot embedding/representation of the discrete label $y$, where the $y$-th entry is 1 and all other entries are zeros. We can also write the loss function in the following way:
  \begin{equation*}
  J(W^{[1]},W^{[2]},b^{[1]},b^{[2]}) = - \frac{1}{\nexp}\sum_{i=1}^\nexp e_{\ysi}^\top\log\left(h_\theta(x^{(i)})\right).
  \end{equation*}
Here $\log(\cdot)$ is applied entry-wise to the vector $h_\theta(\xsi)$. The starter code already converts labels into one-hot representations for you.


Instead of batch gradient descent or stochastic gradient descent, the common practice
is to use mini-batch gradient descent for deep learning tasks. Concretely, we randomly sample $B$ examples $(x^{(i_k)}, y^{(i_k)})_{k=1}^B$ from $(x^{(i)}, y^{(i)})_{i=1}^n$. In this case, the
mini-batch cost function with batch-size $B$ is defined as follows:

  \begin{equation*}
  J_{MB} = \frac{1}{B}\sum_{k=1}^B \ell_\textup{CE}(\bar{h}_{\theta}(x^{(i_k)}),y^{(i_k)})
  \end{equation*}
where $B$ is the batch size, i.e., the number of training examples in each mini-batch.

\begin{enumerate}
  \input{mnist/01-grad}

\ifnum\solutions=1 {
  \input{mnist/01-grad-sol}
} \fi

  \input{mnist/02-unregularized}

\ifnum\solutions=1 {
  \input{mnist/02-unregularized-sol}
} \fi

  \input{mnist/03-regularized}

\ifnum\solutions=1 {
  \input{mnist/03-regularized-sol}
} \fi


  \input{mnist/04-compare}

  \ifnum\solutions=1 {
  \input{mnist/04-compare-sol}
} \fi

%   \input{mnist/05-conv}
% \ifnum\solutions=1 {
%   \input{mnist/05-conv-sol}
% } \fi

 \end{enumerate}

