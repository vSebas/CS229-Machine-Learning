\item \subquestionpoints{10} Show that the unit-length vector $u$ that minimizes the
mean squared error between projected points and original points corresponds
to the first principal component for the data. I.e., show that
$$ \arg \min_{u:u^Tu=1} \sum_{i=1}^\nexp \|x^{(i)}-f_u(x^{(i)})\|_2^2 $$
is equivalent to the optimization problem
$$ \arg \max_{u:u^Tu=1} u^T \left(\sum_{i=1}^\nexp x^{(i)}{x^{(i)}}^T \right) u, $$
which defines the first principal component (the eigenvector corresponding to the largest eigenvalue of $\sum_{i=1}^\nexp x^{(i)}{x^{(i)}}^T$).


{\bf Remark.} If we are asked to find a $k$-dimensional subspace onto which to
project the data so as to minimize the sum of squares distance between the
original data and their projections, then we should choose the $k$-dimensional
subspace spanned by the first $k$ principal components of the data.  This problem
shows that this result holds for the case of $k=1$.