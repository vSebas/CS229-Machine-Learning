\item\subquestionpoints{5}
\textbf{Convergence.}
First we will show that this algorithm eventually converges. In order to prove this, it is sufficient to show that our semi-supervised objective $\ell_\text{semi-sup}(\theta)$ monotonically increases with each iteration of E and M step. Specifically, let $\theta^{(t)}$ be the parameters obtained at the end of $t$ EM-steps. Show that $\ell_\text{semi-sup}(\theta^{(t+1)}) \ge \ell_\text{semi-sup}(\theta^{(t)})$. \textbf{Hint:} Use Jensen's inequality to show that the E-step constructs a lower bound on the 
unsupervised log-likelihood $\ell_{\text{unsup}}(\theta)$, and that the M-step maximizes this bound. 

