\item \points{20} {\bf AdaBoost Performance}

We learned about boosting in lecture. Statistician Kevin Murphy claims that ``It can be shown that, as long as each base learner has an accuracy that is better than chance (even on the weighted dataset), then the final ensemble of classifiers will have higher accuracy than any given component." We will now verify this in the AdaBoost framework.


\begin{enumerate}
    \input{adaboost_performance/01-children}
    
	\ifnum\solutions=1 {
	\input{adaboost_performance/01-children-sol}
        } \fi
        
    \input{adaboost_performance/02-cases}
    
	\ifnum\solutions=1 {
	\input{adaboost_performance/02-cases-sol}
        } \fi
        
    \input{adaboost_performance/03-misclas}
    
	\ifnum\solutions=1 {
	\input{adaboost_performance/03-misclas-sol}
        } \fi
        
\end{enumerate}
